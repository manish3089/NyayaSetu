{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import nltk\n",
    "import re \n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judgements</th>\n",
       "      <th>Area.of.Law</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LNIND_1988_CAL_114</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LNIND_1956_CAL_163</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LNIND_1976_CAL_277</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LNIND_1980_CAL_52</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LNIND_1955_CAL_124</td>\n",
       "      <td>To be Tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>LNIND_1993_DEL_112</td>\n",
       "      <td>Criminal Laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>LNIND_1988_CAL_83</td>\n",
       "      <td>Service Law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>LNIND_1993_DEL_16</td>\n",
       "      <td>Criminal Laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>LNIND_1957_CAL_46</td>\n",
       "      <td>Succession Laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>LNIND_1954_CAL_141</td>\n",
       "      <td>Government Contracts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Judgements           Area.of.Law\n",
       "0    LNIND_1988_CAL_114          To be Tested\n",
       "1    LNIND_1956_CAL_163          To be Tested\n",
       "2    LNIND_1976_CAL_277          To be Tested\n",
       "3     LNIND_1980_CAL_52          To be Tested\n",
       "4    LNIND_1955_CAL_124          To be Tested\n",
       "..                  ...                   ...\n",
       "994  LNIND_1993_DEL_112         Criminal Laws\n",
       "995   LNIND_1988_CAL_83           Service Law\n",
       "996   LNIND_1993_DEL_16         Criminal Laws\n",
       "997   LNIND_1957_CAL_46       Succession Laws\n",
       "998  LNIND_1954_CAL_141  Government Contracts\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look on the labels' file\n",
    "mapping = pd.read_csv('data/Interview_Mapping.csv')\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# create labels\n",
    "unlabeled = []\n",
    "labeled = []\n",
    "labels = []\n",
    "\n",
    "for index,row in mapping.iterrows():\n",
    "    if row['Area.of.Law'] == 'To be Tested':\n",
    "        unlabeled.append(row['Judgements'])\n",
    "    else: \n",
    "        labeled.append(row['Judgements'])\n",
    "        labels.append(row['Area.of.Law'])\n",
    "        \n",
    "# how much unique area of law        \n",
    "print(len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "import os\n",
    "\n",
    "unlabeled_text=[]\n",
    "labeled_text=[]\n",
    "\n",
    "for name in unlabeled:\n",
    "    path = os.path.join('data/',name+'.txt')\n",
    "    with open(path,'r',errors = 'ignore') as f:\n",
    "        unlabeled_text.append(f.read())\n",
    "for name in labeled:\n",
    "    path = os.path.join('data/',name+'.txt')\n",
    "    with open(path,'r',errors = 'ignore') as f:\n",
    "        labeled_text.append(f.read())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labeled_text[] each of the element in this array is a passage\n",
    "and what i need to do is to clean each of the passage first \n",
    "and combine the cleaned passages together into a new array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Flora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Flora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "\n",
    "labeled_cleaned=[]\n",
    "unlabeled_cleaned = []\n",
    "\n",
    "for passage in labeled_text:\n",
    "    # remove links \n",
    "    passage= re.sub(r'http(s)?:\\/\\/\\S*', \"\", str(passage))\n",
    "    # remove \\n\n",
    "    passage = ''.join([elem.replace('\\n',' ') for elem in passage])\n",
    "    # normalization and remove stopwords\n",
    "    passage = ' '.join([elem for elem in passage.lower().split() if elem not in stop])\n",
    "    #remove punctuation \n",
    "    passage = ''.join([elem.replace('[^\\w\\s]',' ') for elem in passage if elem not in punct])\n",
    "    #remove digits\n",
    "    passage = ''.join([elem for elem in passage if not elem.isdigit()])\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    passage = ' '.join(lemmatizer.lemmatize(elem) for elem in passage.split())\n",
    "    \n",
    "    labeled_cleaned.append(passage)\n",
    "\n",
    "    \n",
    "for passage in unlabeled_text:\n",
    "        \n",
    "    # remove links \n",
    "    passage= re.sub(r'http(s)?:\\/\\/\\S*', \"\", str(passage))\n",
    "    # remove \\n\n",
    "    passage = ''.join([elem.replace('\\n',' ') for elem in passage])\n",
    "    # normalization and remove stopwords\n",
    "    passage = ' '.join([elem for elem in passage.lower().split() if elem not in stop])\n",
    "    #remove punctuation \n",
    "    passage = ''.join([elem.replace('[^\\w\\s]',' ') for elem in passage if elem not in punct])\n",
    "    #remove digits\n",
    "    passage = ''.join([elem for elem in passage if not elem.isdigit()])\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    passage = ' '.join(lemmatizer.lemmatize(elem) for elem in passage.split())\n",
    "    unlabeled_cleaned.append(passage)\n",
    "    # it has to be a string so it could be processed later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the labeled data into training and validation set \n",
    "# use 7-3 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_text,val_text,train_labels,val_labels =train_test_split(labeled_cleaned,labels,test_size = 0.25,random_state = 0)\n",
    "\n",
    "# do tfidf to get X_train and X_val\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_text)\n",
    "X_val = vectorizer.transform(val_text)\n",
    "\n",
    "# do tfidf to get X_test (unlabeled text that needed to be predicted)\n",
    "X_test = vectorizer.transform(unlabeled_cleaned) #transform on test set, not fit_transform\n",
    "\n",
    "# do label encoding to get y_train and y_val\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_val = encoder.transform(val_labels)\n",
    "all_labels = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,classification_report\n",
    "\n",
    "def get_metrics(y_val, y_predicted,yHat_train,y_train):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_val, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_val, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_val, y_predicted, pos_label=None, average='weighted')\n",
    "    # true positives + true negatives/ total\n",
    "    accuracyTest = accuracy_score(y_val, y_predicted)\n",
    "    accuracyTrain = accuracy_score(y_train,yHat_train)\n",
    "    return accuracyTest,accuracyTrain, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  6  6  6  6  6  6  6 23  6 23  6 23 39  6  6  6 39  6  6 23 23  6  6\n",
      " 39  6 23  6 39 23  6 39  6 23  6 14  6  6  6  6  6  6  6 23  6  6  6  6\n",
      " 39  6  6 39 39  6  6  6  6  6  6  6 23  6  6  6  6  6  6  6  6  6  6 23\n",
      "  6  6  6  6 23  6  6  6  6 23  6 39  6  6  6  6  6 23 39 23 23  6  6  6\n",
      " 23  6  6  6]\n",
      "Test accuracy = 0.329, Train accuracy = 0.359,precision = 0.250, recall = 0.329, f1 = 0.214\n"
     ]
    }
   ],
   "source": [
    "# 1st model: Naive Bayes\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "def modelNB(X_train,y_train,X_val,X_test):\n",
    "    modelNB = naive_bayes.MultinomialNB()\n",
    "    modelNB.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelNB.predict(X_val)\n",
    "    result_ = modelNB.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelNB.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "predicted_labels,result,yHat_train=modelNB(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 28  6  8 14  1  7 28 23  6 23 39 23 39  6 14 18 39 37 11 23 23 16  7\n",
      " 39 25 23  8 39 36 36 39 34 23 34 14  2 11 28 15 16  6 13 36 34 34 14 38\n",
      " 39 30 13 39 39 27  1 17  1 18  4 11 34 15 34 38 38 35 14 36  6  7 18 36\n",
      "  6 11 37  8 23 13 20 36  6  7  1 39  5 39 34 37 36 23 39 23 23 14 27 16\n",
      " 23 21 13  6]\n",
      "Test accuracy = 0.631, Train accuracy = 0.917,precision = 0.641, recall = 0.631, f1 = 0.623\n"
     ]
    }
   ],
   "source": [
    "# 2nd model: Logistic Regression\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def modelLR(X_train,y_train,X_val,X_test):\n",
    "    modelLR = LogisticRegression(C=3.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=0)\n",
    "    modelLR.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelLR.predict(X_val)\n",
    "    result_ = modelLR.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelLR.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "predicted_labels,result,yHat_train=modelLR(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3nd model: SVM\n",
    "# before applying SVMs,  standardize the data first.\n",
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "from sklearn import decomposition,preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(X_train)\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_val_svd = svd.transform(X_val)\n",
    "X_test_svd=svd.transform(X_test)\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(X_train_svd)\n",
    "X_train_svd_scl = scl.transform(X_train_svd)\n",
    "X_val_svd_scl = scl.transform(X_val_svd)\n",
    "X_test_svd_scl = scl.transform(X_test_svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n",
      "Before SVD: \n",
      "Test accuracy = 0.182, Train accuracy = 0.141,precision = 0.033, recall = 0.182, f1 = 0.056\n",
      "[34  6  6 14  6  6  6 28 23  6 23 39 23 39  6 14  6 39 37  6 23 23  6  6\n",
      " 39  6 23  6 39 36 36 39 34 23 34 14  1 23 14 15 16  6 13 36  6 34 14  6\n",
      " 39 13 13 39 39  6  1 28  1  6  6  6  6  6 34 38 38  6  6 36  6  7  6 36\n",
      "  6  6 37  6 23 13 23 36  6  7  1 39  6 39 34 37 36 23 39 23 23 13  6  6\n",
      " 23 38 13  6]\n",
      "After SVD: \n",
      "Test accuracy = 0.573, Train accuracy = 0.866,precision = 0.521, recall = 0.573, f1 = 0.513\n"
     ]
    }
   ],
   "source": [
    "# use SVM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def modelSVM(X_train,y_train,X_val,X_test):\n",
    "    modelSVM = svm.SVC(C=1.0,probability=True)\n",
    "    modelSVM.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelSVM.predict(X_val)\n",
    "    result_ = modelSVM.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelSVM.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "predicted_labels,result,yHat_train=modelSVM(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Before SVD: \\nTest accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n",
    "\n",
    "\n",
    "predicted_labels_SVD,result_SVD,yHat_train_SVD =modelSVM(X_train_svd_scl,y_train,X_val_svd_scl,X_test_svd_scl)\n",
    "accuracyTest_SVD,accuracyTrain_SVD, precision_SVD, recall_SVD, f1_SVD = get_metrics(y_val, predicted_labels_SVD,yHat_train_SVD,y_train)\n",
    "print(\"After SVD: \\nTest accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest_SVD,accuracyTrain_SVD, precision_SVD, recall_SVD, f1_SVD))\n",
    "\n",
    "# without truncatedSVD, the classifier was somehow underfit.\n",
    "# with truncatedSVD, the accuracy was raised dramatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 28  6  8 28  1 34 28 23  6 23 39 23 39  6 14 37 39 37 34 23 23 37  6\n",
      " 39  6 23  7 39  8 36 39  6 23 34 14  1 34 14 15 37  6 13 36  6 39 14  6\n",
      " 39 14 13 39 39  6  1  6  1 37  6  6 23 14 39 38 34 38 14 36 39 14 37 36\n",
      "  6  6 37  6 23 13  8 14  6  7  1 39 36 39 34 37 36 23 39 23 23 14  6 16\n",
      " 23 21 13  6]\n",
      "1\n",
      "Test accuracy = 0.613, Train accuracy = 0.929,precision = 0.552, recall = 0.613, f1 = 0.558\n"
     ]
    }
   ],
   "source": [
    "# 4th model: XgBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "def modelxgb(X_train,y_train,X_val,X_test):\n",
    "    modelxgb = xgb.XGBClassifier(max_depth=6, n_estimators=200, colsample_bytree=0.5, \n",
    "                        subsample=0.5, nthread=10, learning_rate=0.01)\n",
    "    modelxgb.fit(X_train,y_train)\n",
    "    predicted_labels_ = modelxgb.predict(X_val)\n",
    "    result_ = modelxgb.predict(X_test)\n",
    "    print(result_)\n",
    "    yHat_train_ = modelxgb.predict(X_train)\n",
    "    return predicted_labels_,result_,yHat_train_\n",
    "\n",
    "# see the result on tfidf data \n",
    "predicted_labels,result,yHat_train = modelxgb(X_train,y_train,X_val,X_test)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"1\\nTest accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[34 28  6  8 28  1  4 28 23  6 23  6 23 39  6 14 18 39 37  6 23 23 37  7\n",
    " 39  6 23  7 39  8 36 39  6 23 37 14  2 34 14 15 37  8 13 36 34 34 14  6\n",
    " 39 30 13 39 39  6  2 17  2 18  6  6 23 34 39 38 34 38 14 36 39  7 18 36\n",
    " 27  6 37  6 23 13  8 14  6  7  1 39  6 39 38 37 36 23 39 23 23 13 27 16\n",
    " 23 21 13  6]\n",
    "Before SVD: \n",
    "Test accuracy = 0.618, Train accuracy = 1.000,precision = 0.599, recall = 0.618, f1 = 0.591\n",
    "\n",
    "[34 28  6  8 14  1  6 28 23  6 23 39 23 39  6 14  6 39  6 34 23 23 37  6\n",
    " 39  6 23 28 39 36 36 39  6 23 34 14  1 11 14 15 16  6 13 36 34 34 14 38\n",
    " 39 14 13 39 39 27  1 28  1 18  6 11 23 14 34 38 38  6 14  8 39 14  6 36\n",
    "  6  6  6  6 23 13  6 14  6  7  1 39 28  6 34 37 36 23 39 23 23 14  6 16\n",
    " 23 38 13  6]\n",
    "After SVD: \n",
    "Test accuracy = 0.569, Train accuracy = 1.000,precision = 0.522, recall = 0.569, f1 = 0.525\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try bag of word then tfidf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "train_text,val_text,train_labels,val_labels = train_test_split(labeled_cleaned,labels,test_size = 0.25,random_state = 0)\n",
    "X_train_count, count_vectorizer = cv(train_text)\n",
    "X_val_count = count_vectorizer.transform(val_text)\n",
    "\n",
    "# do tfidf to get X_test (unlabeled text that needed to be predicted)\n",
    "X_test_count = count_vectorizer.transform(unlabeled_cleaned) #transform on test set, not fit_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then do tfidf transformer to make training set and valid. set from occurences to freq.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tf = tfidf_transformer.fit_transform(X_train_count)\n",
    "X_val_tf = tfidf_transformer.transform(X_val_count)\n",
    "X_test_tf = tfidf_transformer.transform(X_test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 28  6  8 14  1  7 28 23  6 23 39 23 39  6 14 18 39 37 11 23 23 16  7\n",
      " 39 25 23  8 39 36 36 39 34 23 34 14  2 11 28 15 16  6 13 36 34 34 14 38\n",
      " 39 30 13 39 39 27  1 17  1 18  4 11 34 15 34 38 38 35 14 36  6  7 18 36\n",
      "  6 11 37  8 23 13 20 36  6  7  1 39  5 39 34 37 36 23 39 23 23 14 27 16\n",
      " 23 21 13  6]\n",
      "Test accuracy = 0.631, Train accuracy = 0.917,precision = 0.641, recall = 0.631, f1 = 0.623\n"
     ]
    }
   ],
   "source": [
    "# see the result in model LR\n",
    "predicted_labels,result,yHat_train=modelLR(X_train_tf,y_train,X_val_tf,X_test_tf)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the result in model XGBoost\n",
    "predicted_labels,result,yHat_train=modelxgb(X_train_tf,y_train,X_val_tf,X_test_tf)\n",
    "accuracyTest,accuracyTrain, precision, recall, f1 = get_metrics(y_val, predicted_labels,yHat_train,y_train)\n",
    "print(\"Test accuracy = %.3f, Train accuracy = %.3f,precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracyTest,accuracyTrain, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we setup a pipeline and do some grid search on LR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "text_clf_LR = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression( random_state=0) ),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__penalty': ('l1','l2'),\n",
    "    'clf__C': (0.01,0.1,1,3,10,100),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.01, 0.1, 1, 3, 10, 100),\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed: 24.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1501.434s\n",
      "\n",
      "Best score: 0.620\n",
      "Best parameters set:\n",
      "\tclf__C: 100\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(text_clf_LR, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf_LR.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_text, train_labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_LR = Pipeline([('vect', CountVectorizer(max_df=1.0,ngram_range=(1,2))),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(C=100,penalty='l2',random_state=0)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222222222222222"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_LR.fit(train_text, y_train)  \n",
    "predicted_ed2 = text_clf_LR.predict(val_text)\n",
    "np.mean(predicted_ed2 == y_val)    \n",
    "\n",
    "# which doesn't improve alot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try grid search on xgboost\n",
    "text_clf_xgb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('clf', xgb.XGBClassifier(random_state=0) ),\n",
    " ])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_depth': (3,9,15),\n",
    "    'clf__alpha': (0,0.1,0.5,1),\n",
    "    'clf__Eta':(0.01,0.015,0.05,0.1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__Eta': (0.01, 0.015, 0.05, 0.1),\n",
      " 'clf__alpha': (0, 0.1, 0.5, 1),\n",
      " 'clf__max_depth': (3, 9, 15),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(text_clf_xgb, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf_xgb.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_text, train_labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# write result in csv\n",
    "with open('predictions.csv','w') as f:\n",
    "    f.write('Judgements' + '\\t' + 'Area of Law' + '\\n')\n",
    "    predictionList = all_labels[result]\n",
    "    for i in range(0, len(result)):\n",
    "        f.write(unlabeled[i] + '\\t' + predictionList[i] + '\\n')\n",
    "        \n",
    "sss = pd.read_csv('predictions.csv')\n",
    "print(sss)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
